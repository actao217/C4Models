# -*- coding: utf-8 -*-
"""C4_CNN_Network_A_Policy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IAutJMeGPMZxT7Rdqf33gh31EZj72_1t
"""

import numpy as np

class Board:
  rows = 6
  cols = 7
  empty = '.'

  def __init__(self):
    self.board = [[Board.empty for _ in range(self.cols)]
                  for _ in range(self.rows)]

  def drop(self, col, piece):
    # drops piece into column, returns true if four in a row is achieved
    if not (0 <= col < self.cols):
      raise ValueError('Column out of bounds') # issue with previous models has been attempt of illegal moves
    for row in range(self.rows):
      if self.board[row][col] == Board.empty:
        self.board[row][col] = piece
        return self.check_win(row, col, piece)
    raise ValueError('Column is full')

  def check_win(self, row, col, piece):
    # checks if placing a piece results in a four in a row using BFS
    return (self._count(col, row, 1, 0, piece) + self._count(col, row, -1, 0, piece) - 1 >= 4 or
            self._count(col, row, 0, 1, piece) + self._count(col, row, 0, -1, piece) - 1 >= 4 or
            self._count(col, row, 1, 1, piece) + self._count(col, row, -1, -1, piece) - 1 >= 4 or
            self._count(col, row, 1, -1, piece) + self._count(col, row, -1, 1, piece) - 1 >= 4)

  def _count(self, col, row, dc, dr, piece):
    c, r = col, row
    count = 0
    while 0 <= c < self.cols and  0 <= r < self.rows and self.board[r][c] == piece:
      count += 1
      c += dc
      r += dr
    return count

  def get_valid_moves(self):
    # returns a list of columns where a piece can be dropped
    valid = []
    for c in range(self.cols):
      if self.board[self.rows - 1][c] == Board.empty:
        valid.append(c)
    return valid

# allows for replication of games/training
  def clone(self):
    new_b = Board()
    new_b.board = [row[:] for row in self.board]
    return new_b

class Game:
  # two player game with players 'X' and 'O', with alternating turns
  def __init__(self):
    self.board = Board()
    self.current_player = 'X'
    self.winner = None
    self.finished = False

  def move(self, col):
    if self.finished:
      raise ValueError('Game is already finished')
    if self.board.drop(col, self.current_player):
      self.winner = self.current_player
      self.finished = True
    else:
      # checking for draw
      if len(self.board.get_valid_moves()) == 0:
        self.finished = True
        self.winner = None
    # if game isn't over, swap players
    self.current_player = 'O' if self.current_player == 'X' else 'X'

  def clone(self):
    g = Game()
    g.board = self.board.clone()
    g.current_player = self.current_player
    g.winner = self.winner
    g.finished = self.finished
    return g

import tensorflow as tf

from tensorflow.keras import layers, models, optimizers

class NetworkA:
  """
  CNN with:
    - Conv2D(64, 2x2), ReLU
    - Conv2D(64, 2x2), ReLU
    - Flatten
    - Dense(64), ReLU
    - Dense(64), ReLU
    - Dense(1) [scalar value output]
  """
  def __init__(self, learning_rate=1e-3):
        self.model = models.Sequential()
        self.model.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape=(6, 7, 1)))
        self.model.add(layers.Conv2D(64, (2, 2), activation='relu'))
        self.model.add(layers.Flatten())
        self.model.add(layers.Dense(64, activation='relu'))
        self.model.add(layers.Dense(64, activation='relu'))
        self.model.add(layers.Dense(1))  # single scalar output
        self.model.compile(loss='mean_squared_error', optimizer=optimizers.Adam(learning_rate=learning_rate)
        )

  def predict(self, boards):
    # boards: shape (batch_size, 6, 7)
    boards = boards[..., np.newaxis] # expand to (batch_size, 6, 7, 1)
    return self.model(boards, training=False).numpy()

  def train_on_batch(self, boards, targets):
    boards = boards[..., np.newaxis]
    return self.model.train_on_batch(boards, targets)

  def save(self, filepath="networkA.h5"):
    self.model.save(filepath)

  def load(self, filepath="networkA.h5"):
    self.model = tf.keras.models.load_model(filepath)

def encode_board(board, current_player):
    """
    Convert a board into a (6,7) array of +1/–1/0 from the current player's perspective.
    """
    mat = []
    opponent = 'O' if current_player == 'X' else 'X'
    for row in board.board:
        row_enc = []
        for cell in row:
            if cell == current_player:
                row_enc.append(1)
            elif cell == opponent:
                row_enc.append(-1)
            else:
                row_enc.append(0)
        mat.append(row_enc)
    return np.array(mat, dtype=np.float32)

import random

class NNStrategy:
    def __init__(self, network, discount_factor=0.9, exploration_rate=0.1, batch_size=64):
        self.network = network
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.batch_size = batch_size

        # memory of board states for *this* game (resets every new game)
        self.current_game_states = []

        # larger memory for training in mini-batches
        self.train_states = []
        self.train_targets = []

    def pick_move(self, game):
      valid_moves = game.board.get_valid_moves()

      # epsilon-greedy exploration
      if random.random() < self.exploration_rate:
          return random.choice(valid_moves)

      # 1) gathers the board states and encodes for each valid move
      states_batch = []
      for mv in valid_moves:
          clone_g = game.clone()
          clone_g.move(mv)
          enc = encode_board(clone_g.board, clone_g.current_player)
          states_batch.append(enc)

      # 2) convert to a NumPy array
      states_arr = np.array(states_batch, dtype=np.float32)  # shape (N, 6, 7)

      # 3) single forward pass
      values = self.network.predict(states_arr).flatten()

      # 4) picks move with highest predicted value
      best_idx = np.argmax(values)
      best_move = valid_moves[best_idx]
      return best_move

    def record_state(self, game):
        """
        after picking a move, we call this to store the new board state
        from the perspective of the *next* player.
        """
        enc = encode_board(game.board, game.current_player)
        self.current_game_states.append(enc)

    def game_over(self, winner, last_player):
        """
        gives +1 to the winner, -1 to loser, or 0 for a draw, and discount that
        reward back through the game states.
        """
        if winner is None:
            reward = 0
        elif winner == last_player:
            reward = +1
        else:
            reward = -1

        # traverses states backward, assigning discounted reward
        for state in reversed(self.current_game_states):
            self.train_states.append(state)
            self.train_targets.append(reward)
            reward *= self.discount_factor

        # clear this game’s memory
        self.current_game_states.clear()

        # train in a batch
        if len(self.train_targets) >= self.batch_size:
            states_arr = np.array(self.train_states, dtype=np.float32)
            targets_arr = np.array(self.train_targets, dtype=np.float32)

            self.network.train_on_batch(states_arr, targets_arr)
            self.train_states.clear()
            self.train_targets.clear()

    def save(self, filepath = "networkA.h5"):
        self.network.save(filepath)

def random_move(game):
    valid = game.board.get_valid_moves()
    if not valid:
        return None
    return random.choice(valid)

def train_vs_random(num_episodes=150_000, batch_size=64):
    """
    Plays 'num_episodes' Connect 4 games where:
      X = NNStrategy (using NetworkA)
      O = random.
    Tracks X's win rate every 250 games.
    Returns both the results list and the trained agent.
    """
    netA = NetworkA(learning_rate=1e-3)
    agentA = NNStrategy(netA, discount_factor=0.9, exploration_rate=0.2, batch_size=batch_size)

    results = []
    x_wins_in_block = 0

    from tqdm import trange
    pbar = trange(num_episodes, desc="Training vs Random", leave=True)

    for episode in pbar:
        game = Game()
        if random.random() < 0.5:
          game.current_player = 'X'
        else:
          game.current_player = 'O'

        while not game.finished:
            if game.current_player == 'X':
                move_x = agentA.pick_move(game)
                game.move(move_x)
                agentA.record_state(game)
            else:
                move_o = random_move(game)
                game.move(move_o)

        winner = game.winner
        last_player = 'O' if game.current_player == 'X' else 'X'
        agentA.game_over(winner, last_player)

        if winner == 'X':
            x_wins_in_block += 1

        if (episode + 1) % 250 == 0:
            results.append((episode + 1, x_wins_in_block))
            x_wins_in_block = 0

    return results, agentA  # return saveable agent

import matplotlib.pyplot as plt

def plot_results(results):
    """
    results: list of (games_so_far, x_wins_in_block).
    """
    xs = [item[0] for item in results]
    # number of wins wins in blocks of 250
    ys = [item[1] / 250.0 for item in results]

    plt.figure(figsize=(8,6))
    plt.plot(xs, ys, label="X's Win Rate (per 250 games)")
    plt.xlabel("Games Played")
    plt.ylabel("Win Rate in Last 250")
    plt.ylim(0, 1)
    plt.title("NN vs Random Over Time")
    plt.legend()
    plt.show()

# actually run 150,000 training games:
results, trained_agentA = train_vs_random(num_episodes=150_000)
trained_agentA.network.model.save("networkA_150k.h5")

plot_results(results)

plot_results(results)

import cProfile
import pstats

def run_profile():
    # your training code, e.g.
    results = train_vs_random(num_episodes=10)

cProfile.run('run_profile()', 'profile_stats')
p = pstats.Stats('profile_stats')
p.strip_dirs().sort_stats('cumtime').print_stats(20)