# -*- coding: utf-8 -*-
"""C4_CNN_NetworkA_vs_MCTS50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uc3o144-6dOsBV3TARi7U_ba04I-glNN
"""

import numpy as np
import random
import tensorflow as tf
tf.config.run_functions_eagerly(True)
from tensorflow.keras.models import load_model
from tensorflow.keras import layers, models, optimizers
from collections import Counter
from tqdm import trange

# --- Game Environment ---
class Board:
    rows = 6
    cols = 7
    empty = '.'

    def __init__(self):
        self.board = [[Board.empty for _ in range(self.cols)] for _ in range(self.rows)]

    def drop(self, col, piece):
        for row in range(self.rows):
            if self.board[row][col] == Board.empty:
                self.board[row][col] = piece
                return self.check_win(row, col, piece)
        raise ValueError('Column full')

    def check_win(self, row, col, piece):
        return (
            self._count(col, row, 1, 0, piece) + self._count(col, row, -1, 0, piece) - 1 >= 4 or
            self._count(col, row, 0, 1, piece) + self._count(col, row, 0, -1, piece) - 1 >= 4 or
            self._count(col, row, 1, 1, piece) + self._count(col, row, -1, -1, piece) - 1 >= 4 or
            self._count(col, row, 1, -1, piece) + self._count(col, row, -1, 1, piece) - 1 >= 4
        )

    def _count(self, col, row, dc, dr, piece):
        c, r = col, row
        count = 0
        while 0 <= c < self.cols and 0 <= r < self.rows and self.board[r][c] == piece:
            count += 1
            c += dc
            r += dr
        return count

    def get_valid_moves(self):
        return [c for c in range(self.cols) if self.board[self.rows - 1][c] == Board.empty]

    def clone(self):
        b = Board()
        b.board = [row[:] for row in self.board]
        return b

class Game:
    def __init__(self):
        self.board = Board()
        self.current_player = 'X'
        self.winner = None
        self.finished = False

    def move(self, col):
        if self.finished:
            raise ValueError("Game over")
        if self.board.drop(col, self.current_player):
            self.winner = self.current_player
            self.finished = True
        elif not self.board.get_valid_moves():
            self.finished = True
            self.winner = None
        self.current_player = 'O' if self.current_player == 'X' else 'X'

    def clone(self):
        g = Game()
        g.board = self.board.clone()
        g.current_player = self.current_player
        g.winner = self.winner
        g.finished = self.finished
        return g

# --- Utilities ---
def encode_board(board, current_player):
    opponent = 'O' if current_player == 'X' else 'X'
    return np.array([
        [1 if c == current_player else -1 if c == opponent else 0 for c in row]
        for row in board.board
    ], dtype=np.float32)

def random_move(game):
    return random.choice(game.board.get_valid_moves())

# --- CNN Architecture ---
class NetworkA:
    def __init__(self, learning_rate=1e-3):
        self.model = models.Sequential([
            layers.Conv2D(64, (2,2), activation='relu', input_shape=(6,7,1)),
            layers.Conv2D(64, (2,2), activation='relu'),
            layers.Flatten(),
            layers.Dense(64, activation='relu'),
            layers.Dense(64, activation='relu'),
            layers.Dense(1)
        ])
        self.model.compile(loss='mean_squared_error',
                           optimizer=optimizers.Adam(learning_rate=learning_rate))

    def predict(self, boards):
        boards = boards[..., np.newaxis]
        return self.model(boards, training=False).numpy()

    def train_on_batch(self, boards, targets):
        boards = boards[..., np.newaxis]
        return self.model.train_on_batch(boards, targets)

# --- CNN Strategy ---
class NNStrategy:
    def __init__(self, network, discount_factor=0.9, exploration_rate=0.1, batch_size=64):
        self.network = network
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.batch_size = batch_size
        self.train_states = []
        self.train_targets = []
        self.current_game_states = []

    def pick_move(self, game):
        valid_moves = game.board.get_valid_moves()
        if random.random() < self.exploration_rate:
            return random.choice(valid_moves)

        states = []
        for mv in valid_moves:
            g = game.clone()
            g.move(mv)
            states.append(encode_board(g.board, g.current_player))
        arr = np.array(states, dtype=np.float32)
        preds = self.network.predict(arr).flatten()
        return valid_moves[np.argmax(preds)]

    def record_state(self, game):
        encoded = encode_board(game.board, game.current_player)
        self.current_game_states.append(encoded)

    def game_over(self, winner, last_player):
        if winner is None:
            reward = 0
        elif winner == last_player:
            reward = +1
        else:
            reward = -1

        for state in reversed(self.current_game_states):
            self.train_states.append(state)
            self.train_targets.append(reward)
            reward *= self.discount_factor

        self.current_game_states.clear()

        if len(self.train_states) >= self.batch_size:
            states = np.array(self.train_states, dtype=np.float32)
            rewards = np.array(self.train_targets, dtype=np.float32)
            self.network.train_on_batch(states, rewards)
            self.train_states.clear()
            self.train_targets.clear()

# --- MCTS Opponent ---
class MCTSStrategy:
    def __init__(self, rollout_limit=50):
        self.rollout_limit = rollout_limit

    def pick_move(self, game):
        from collections import defaultdict
        move_scores = defaultdict(int)
        valid_moves = game.board.get_valid_moves()
        for move in valid_moves:
            for _ in range(self.rollout_limit):
                g = game.clone()
                g.move(move)
                result = self.simulate(g)
                if result == game.current_player:
                    move_scores[move] += 1
        return max(valid_moves, key=lambda m: move_scores[m])

    def simulate(self, game):
        while not game.finished:
            mv = random_move(game)
            game.move(mv)
        return game.winner

# NetworkWrapper so NNStrategy can use a loaded Keras model
class NetworkWrapper:
    def __init__(self, model):
        self.model = model

    def predict(self, boards):
        boards = boards[..., tf.newaxis]
        # model(...) is fine; returns a Tensor
        return self.model(boards, training=False).numpy()

    def train_on_batch(self, boards, targets):
        boards = boards[..., tf.newaxis]
        return self.model.train_on_batch(boards, targets)

# --- Training Loop: CNN vs MCTS-50 with Checkpoints ---
def train_vs_mcts(model_file, num_games=100_000, checkpoint_every=10_000):
    """
    model_file: path to a .h5 (or .keras) file with your initial CNN weights
    num_games: total self-play games vs MCTS
    checkpoint_every: frequency to save intermediate checkpoints
    """

    # 1) load and recompile
    model = load_model(model_file)
    model.compile(loss='mean_squared_error', optimizer=optimizers.Adam(learning_rate=1e-3))

    # 2) wrap the model for NNStrategy
    cnn = NNStrategy(NetworkWrapper(model))

    # 3) create MCTS
    mcts = MCTSStrategy(rollout_limit=50)

    # 4) tracking results in { 'cnn': #wins, 'mcts': #wins, 'draw': # }
    results = Counter({'cnn': 0, 'mcts': 0, 'draw': 0})

    # 5) loop over games
    for i in trange(1, num_games + 1, desc="Training vs MCTS-50"):
        game = Game()

        # Randomly assign which side is CNN vs MCTS
        # if i is even: X=cnn, O=mcts
        # if i is odd:  X=mcts, O=cnn
        if i % 2 == 0:
            player_map = {'X': cnn, 'O': mcts}
        else:
            player_map = {'X': mcts, 'O': cnn}

        while not game.finished:
            strategy = player_map[game.current_player]
            move = strategy.pick_move(game)
            game.move(move)
            # if that strategy is the NN, record state
            if isinstance(strategy, NNStrategy):
                strategy.record_state(game)

        winner = game.winner  # 'X', 'O', or None (draw)
        last_player = 'O' if game.current_player == 'X' else 'X'

        # tell the NN about end of game
        for side, strategy in player_map.items():
            if isinstance(strategy, NNStrategy):
                strategy.game_over(winner, last_player)

        # 6) Which side actually won?
        # If winner == 'X', then if X=cnn => cnn++  else => mcts++
        # If winner == 'O', then if O=cnn => cnn++  else => mcts++
        # If draw => results['draw']++
        if winner is None:
            results['draw'] += 1
        else:
            if winner == 'X':
                if isinstance(player_map['X'], NNStrategy):
                    results['cnn'] += 1
                else:
                    results['mcts'] += 1
            else:  # winner == 'O'
                if isinstance(player_map['O'], NNStrategy):
                    results['cnn'] += 1
                else:
                    results['mcts'] += 1

        # 7) checkpoint saving
        if i % checkpoint_every == 0:
            checkpoint_path = f"networkA_checkpoint_{i}.h5"
            model.save(checkpoint_path)  # or model.save("my_model.keras") if you prefer
            print(f"\nðŸ’¾ Checkpoint saved at game {i} â†’ {checkpoint_path}")
            print(f"ðŸ“Š So far â†’ CNN wins: {results['cnn']}, MCTS wins: {results['mcts']}, Draws: {results['draw']}")

    return model, results

if __name__ == "__main__":
    final_model, final_results = train_vs_mcts(
        model_file="networkA_150k.h5",   # your starting model
        num_games=100_000,              # only 100k now
        checkpoint_every=10_000
    )

    # Final save
    final_model.save("networkA_trained_vs_mcts50.h5")
    print("\nâœ… Final model saved as networkA_trained_vs_mcts50.h5")

    # Print final results
    print("\n=== Final Results ===")
    print(f"CNN Wins:   {final_results['cnn']}")
    print(f"MCTS Wins:  {final_results['mcts']}")
    print(f"Draws:      {final_results['draw']}")